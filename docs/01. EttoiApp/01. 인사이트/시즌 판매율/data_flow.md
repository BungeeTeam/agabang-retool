# Data Flow Documentation

**ìƒì„±ì¼ì‹œ:** 2025-10-01 11:21:06 KST  
**ì†ŒìŠ¤ ë””ë ‰í† ë¦¬:** `apps/01.%20EttoiApp/01.%20%EC%9D%B8%EC%82%AC%EC%9D%B4%ED%8A%B8/%EC%8B%9C%EC%A6%8C%20%ED%8C%90%EB%A7%A4%EC%9C%A8/lib`  
**ë¶„ì„ëœ íŒŒì¼ ìˆ˜:** 43ê°œ  
**ë¶„ì„ ë„êµ¬:** OpenAI GPT-3.5-turbo

## ğŸ“Š ì½”ë“œ ë¶„ì„ ê²°ê³¼

ì½”ë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: Error code: 400 - {'error': {'message': "This model's maximum context length is 16385 tokens. However, you requested 17142 tokens (13142 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}

---

## ğŸ” ë¶„ì„ ìš”ì•½

- **ë¶„ì„ ëŒ€ìƒ:** apps/01.%20EttoiApp/01.%20%EC%9D%B8%EC%82%AC%EC%9D%B4%ED%8A%B8/%EC%8B%9C%EC%A6%8C%20%ED%8C%90%EB%A7%A4%EC%9C%A8/lib ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì†ŒìŠ¤ì½”ë“œ
- **ì²˜ë¦¬ëœ íŒŒì¼:** 43ê°œ
- **ë¶„ì„ ë„êµ¬:** OpenAI GPT-3.5-turbo
- **ë¬¸ì„œ ìƒì„±ì¼ì‹œ:** 2025-10-01 11:21:06 KST

---
*ì´ ë¬¸ì„œëŠ” docScribe Code Analyzerì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
